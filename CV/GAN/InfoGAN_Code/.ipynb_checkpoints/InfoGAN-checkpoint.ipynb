{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SharePart(nn.Module):\n",
    "    \n",
    "    ''' front end part of discriminator and Q'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SharePart, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "          nn.Conv2d(1, 64, 4, 2, 1),\n",
    "          nn.LeakyReLU(0.1, inplace=True),\n",
    "          nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.LeakyReLU(0.1, inplace=True),\n",
    "          nn.Conv2d(128, 1024, 7, bias=False),\n",
    "          nn.BatchNorm2d(1024),\n",
    "          nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x).view(-1, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Q(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Q, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(1024, 128, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.conv_disc = nn.Conv2d(128, 10, 1)\n",
    "        self.conv_mu = nn.Conv2d(128, 2, 1)\n",
    "        self.conv_var = nn.Conv2d(128, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.conv(x)\n",
    "        disc_logits = self.conv_disc(y).squeeze()\n",
    "\n",
    "        mu = self.conv_mu(y).squeeze()\n",
    "        var = self.conv_var(y).squeeze().exp()\n",
    "\n",
    "        return disc_logits, mu, var \n",
    "\n",
    "\n",
    "class G(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "          nn.ConvTranspose2d(74, 1024, 1, 1, bias=False),\n",
    "          nn.BatchNorm2d(1024),\n",
    "          nn.ReLU(True),\n",
    "          nn.ConvTranspose2d(1024, 128, 7, 1, bias=False),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(True),\n",
    "          nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(True),\n",
    "          nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "         m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class log_gaussian:\n",
    "    \n",
    "    def __call__(self, x, mu, var):\n",
    "        logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - \\\n",
    "        (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "        return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    # Initialize\n",
    "    def __init__(self, G, SP, D, Q):\n",
    "        self.G = G\n",
    "        self.SP = SP\n",
    "        self.D = D\n",
    "        self.Q = Q\n",
    "\n",
    "        self.batch_size = 100\n",
    "\n",
    "    # sampling from noise tensor\n",
    "    def _noise_sample(self, dis_c, con_c, noise, bs):\n",
    "        idx = np.random.randint(10, size=bs)\n",
    "        c = np.zeros((bs, 10))\n",
    "        c[range(bs),idx] = 1.0\n",
    "        \n",
    "        dis_c.data.copy_(torch.Tensor(c))\n",
    "        con_c.data.uniform_(-1.0, 1.0)\n",
    "        noise.data.uniform_(-1.0, 1.0)\n",
    "        z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "\n",
    "        return z, idx\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        real_x = torch.FloatTensor(self.batch_size, 1, 28, 28).cuda()\n",
    "        label = torch.FloatTensor(self.batch_size, 1).cuda()\n",
    "        dis_c = torch.FloatTensor(self.batch_size, 10).cuda()\n",
    "        con_c = torch.FloatTensor(self.batch_size, 2).cuda()\n",
    "        noise = torch.FloatTensor(self.batch_size, 62).cuda()\n",
    "\n",
    "\n",
    "        criterionD = nn.BCELoss().cuda()\n",
    "        criterionQ_dis = nn.CrossEntropyLoss().cuda()\n",
    "        criterionQ_con = log_gaussian()\n",
    "\n",
    "        optimD = optim.Adam([{'params':self.SP.parameters()}, {'params':self.D.parameters()}], lr=0.0002, betas=(0.5, 0.99))\n",
    "        optimG = optim.Adam([{'params':self.G.parameters()}, {'params':self.Q.parameters()}], lr=0.001, betas=(0.5, 0.99))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Dataset Load\n",
    "        transforms_train = transforms.Compose([\n",
    "            transforms.Resize(28),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5],[0.5])\n",
    "        ])\n",
    "\n",
    "        dataset = dset.MNIST(\"./\",\n",
    "                                 train=True,\n",
    "                                transform = transforms_train,\n",
    "                                download=True)\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   drop_last=True,\n",
    "                                                   num_workers=8)\n",
    "        \n",
    "\n",
    "        # fixed random variables\n",
    "        c = np.linspace(-1, 1, 10).reshape(1, -1)\n",
    "        c = np.repeat(c, 10, 0).reshape(-1, 1)\n",
    "\n",
    "        c1 = np.hstack([c, np.zeros_like(c)])\n",
    "        c2 = np.hstack([np.zeros_like(c), c])\n",
    "\n",
    "        idx = np.arange(10).repeat(10)\n",
    "        one_hot = np.zeros((100, 10))\n",
    "        one_hot[range(100), idx] = 1\n",
    "        fix_noise = torch.Tensor(100, 62).uniform_(-1, 1)\n",
    "\n",
    "\n",
    "        for epoch in range(100):\n",
    "              for num_iters, batch_data in enumerate(dataloader, 0):\n",
    "                    \n",
    "                    # real part\n",
    "                    optimD.zero_grad()\n",
    "\n",
    "                    x, _ = batch_data\n",
    "\n",
    "                    bs = x.size(0)\n",
    "                    real_x.data.resize_(x.size())\n",
    "                    label.data.resize_(bs, 1)\n",
    "                    dis_c.data.resize_(bs, 10)\n",
    "                    con_c.data.resize_(bs, 2)\n",
    "                    noise.data.resize_(bs, 62)\n",
    "\n",
    "                    real_x.data.copy_(x)\n",
    "                    sp_out1 = self.SP(real_x)\n",
    "                    probs_real = self.D(sp_out1)\n",
    "                    label.data.fill_(1)\n",
    "                    loss_real = criterionD(probs_real, label)\n",
    "                    loss_real.backward()\n",
    "\n",
    "                    # fake part\n",
    "                    z, idx = self._noise_sample(dis_c, con_c, noise, bs)\n",
    "                    fake_x = self.G(z)\n",
    "                    sp_out2 = self.SP(fake_x.detach())\n",
    "                    probs_fake = self.D(sp_out2)\n",
    "                    label.data.fill_(0)\n",
    "                    loss_fake = criterionD(probs_fake, label)\n",
    "                    loss_fake.backward()\n",
    "\n",
    "                    D_loss = loss_real + loss_fake\n",
    "\n",
    "                    optimD.step()\n",
    "\n",
    "                    # G and Q part\n",
    "                    optimG.zero_grad()\n",
    "\n",
    "                    sp_out = self.SP(fake_x)\n",
    "                    probs_fake = self.D(sp_out)\n",
    "                    label.data.fill_(1.0)\n",
    "\n",
    "                    reconstruct_loss = criterionD(probs_fake, label)\n",
    "\n",
    "                    q_logits, q_mu, q_var = self.Q(sp_out)\n",
    "                    class_ = torch.LongTensor(idx).cuda()\n",
    "                    target = Variable(class_)\n",
    "                    dis_loss = criterionQ_dis(q_logits, target)\n",
    "                    con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1\n",
    "\n",
    "                    G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "                    G_loss.backward()\n",
    "                    optimG.step()\n",
    "\n",
    "                    if num_iters % 100 == 0:\n",
    "                        print('Epoch/Iter:{0}/{1}, Dloss: {2}, Gloss: {3}'.format(epoch, num_iters, D_loss.data.cpu().numpy(),G_loss.data.cpu().numpy()))\n",
    "                        noise.data.copy_(fix_noise)\n",
    "                        dis_c.data.copy_(torch.Tensor(one_hot))\n",
    "\n",
    "                        con_c.data.copy_(torch.from_numpy(c1))\n",
    "                        z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "                        x_save = self.G(z)\n",
    "                        save_image(x_save.data, './tmp/c1_epoch{0}_iter{1}.png'.format(epoch,num_iters), nrow=10)\n",
    "\n",
    "                        con_c.data.copy_(torch.from_numpy(c2))\n",
    "                        z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "                        x_save = self.G(z)\n",
    "                        save_image(x_save.data, './tmp/c2_epoch{0}_iter{1}.png'.format(epoch, num_iters), nrow=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a76704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sp = SharePart()\n",
    "d = D()\n",
    "q = Q()\n",
    "g = G()\n",
    "\n",
    "for i in [sp, d, q, g]:\n",
    "    i.cuda()\n",
    "    i.apply(weights_init)\n",
    "\n",
    "trainer = Trainer(g, sp, d, q)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345e710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
